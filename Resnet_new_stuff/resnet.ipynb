{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  FINAL SCRIPT: DENSE PAIRING + REGRESSION LOSS (8-DIM)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  1. Dataset (Expects Pairs)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "class WearPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sample format: (img1_path, img2_path, w1_vec, w2_vec)\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, transform):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path1, path2, w1, w2 = self.samples[idx]\n",
    "\n",
    "        img1 = self.transform(Image.open(path1).convert(\"RGB\"))\n",
    "        img2 = self.transform(Image.open(path2).convert(\"RGB\"))\n",
    "\n",
    "        return (\n",
    "            img1,\n",
    "            img2,\n",
    "            torch.tensor(w1, dtype=torch.float32),\n",
    "            torch.tensor(w2, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  2. Model (One Backbone, Multiple Heads)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "class MultiHeadWearNet(nn.Module):\n",
    "    def __init__(self, num_heads=3, embed_dim=8, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared Backbone\n",
    "        backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        feat_dim = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Independent Heads (Linear Layers)\n",
    "        # We do NOT normalize here because we need the output magnitude \n",
    "        # to match the physical wear distance (mm).\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(feat_dim, embed_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, head_idx=None):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # If specific head requested (during inference or single-head tasks)\n",
    "        if head_idx is not None:\n",
    "            return self.heads[head_idx](features)\n",
    "        \n",
    "        # Return all for training loop\n",
    "        return [head(features) for head in self.heads]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  3. Loss (Equation 1: Regression)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "class WearDistanceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimizes: ( ||E1 - E2|| - |w1 - w2| )^2\n",
    "    \"\"\"\n",
    "    def forward(self, E1, E2, w1, w2):\n",
    "        # Euclidean distance in embedding space\n",
    "        d_embed = torch.norm(E1 - E2, p=2, dim=1)\n",
    "        \n",
    "        # Absolute difference in wear (ground truth)\n",
    "        d_wear = torch.abs(w1 - w2)\n",
    "        \n",
    "        # MSE\n",
    "        return ((d_embed - d_wear) ** 2).mean()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  4. Setup & Dense Pairing Logic\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# --- USER DATA INPUT AREA ---\n",
    "# You need to parse your folders to fill these two lists.\n",
    "# sequence_paths: List[List[str]] -> [ [seq1_img0, seq1_img1...], [seq2_img0...] ]\n",
    "# sequence_wears: List[List[List[float]]] -> [ [[0.1, 0.2, 0.3], ...], ... ]\n",
    "# (Make sure wears are 3D vectors: [Flank, Adhesion, F+A])\n",
    "\n",
    "sequence_paths = [] \n",
    "sequence_wears = []\n",
    "# Example filler (Remove this when you add your real loading logic):\n",
    "# sequence_paths = [[\"p1.jpg\", \"p2.jpg\", \"p3.jpg\"]]\n",
    "# sequence_wears = [[[0.1, 0.0, 0.1], [0.2, 0.0, 0.2], [0.5, 0.1, 0.6]]]\n",
    "\n",
    "samples = []\n",
    "\n",
    "print(\"Generating Dense Pairs...\")\n",
    "# DENSE PAIRING LOOP (All vs All within sequence)\n",
    "for paths, wears in zip(sequence_paths, sequence_wears):\n",
    "    N = len(paths)\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N): # j > i ensures unique pairs and no self-pairs\n",
    "            # Append tuple: (path_i, path_j, wear_i, wear_j)\n",
    "            samples.append((paths[i], paths[j], wears[i], wears[j]))\n",
    "\n",
    "print(f\"Generated {len(samples)} pairs.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  5. Training Loop\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if len(samples) > 0:\n",
    "    dataset = WearPairDataset(samples, transform)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Initialize Model (One Backbone, 3 Heads, 8-Dim output)\n",
    "    model = MultiHeadWearNet(num_heads=3, embed_dim=8, freeze_backbone=True).to(device)\n",
    "    optimizer = torch.optim.Adam(model.heads.parameters(), lr=1e-3)\n",
    "    criterion = WearDistanceLoss()\n",
    "\n",
    "    epochs = 20\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = [0.0, 0.0, 0.0]\n",
    "        batch_count = 0\n",
    "\n",
    "        for img1, img2, w1, w2 in loader:\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            w1, w2 = w1.to(device), w2.to(device) # [B, 3]\n",
    "\n",
    "            # 1. Forward Pass (Backbone runs once per batch)\n",
    "            # We get a list of 3 embeddings for img1 and img2\n",
    "            out1 = model(img1) # List of 3 tensors\n",
    "            out2 = model(img2) # List of 3 tensors\n",
    "\n",
    "            batch_loss = 0.0\n",
    "            \n",
    "            # 2. Compute Loss per Head\n",
    "            for k in range(3):\n",
    "                # out1[k] is the embedding for head k\n",
    "                # w1[:, k] is the scalar wear value for head k\n",
    "                loss_k = criterion(out1[k], out2[k], w1[:, k], w2[:, k])\n",
    "                \n",
    "                batch_loss += loss_k\n",
    "                total_loss[k] += loss_k.item()\n",
    "\n",
    "            # 3. Backprop (Sum of losses)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_count += 1\n",
    "\n",
    "        # Logging\n",
    "        avg_losses = [l / batch_count for l in total_loss] if batch_count > 0 else [0]*3\n",
    "        print(f\"Epoch {epoch+1:02d} | L_Flank: {avg_losses[0]:.4f} | L_Adh: {avg_losses[1]:.4f} | L_FA: {avg_losses[2]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No samples found. Please populate sequence_paths and sequence_wears.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
